{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import canvas_interface\n",
    "import lesson_content\n",
    "from datetime import date\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today().strftime('%m_%d_%Y')\n",
    "file_location = 'https://raw.githubusercontent.com/saturncloud/flatiron-curriculum/main/links.csv'\n",
    "file_raw = requests.get(file_location)\n",
    "file_text = file_raw.text\n",
    "df_raw = pd.read_csv(file_text, index_col=0)\n",
    "#with open(f'links_updated_{today}.csv', 'w') as f:\n",
    "    #f.write(file_text)\n",
    "\n",
    "# create a dataframe of the links for SaturnCloud and remove the notebook file from the name for reference\n",
    "#df = pd.read_csv(f'links_updated_{today}.csv', index_col=0)\n",
    "df = df_raw\n",
    "\n",
    "df['local_path'] = df['local_path'].apply(lambda x: x.split('/')[1])\n",
    "df_final = df.drop_duplicates(subset=['local_path'])\n",
    "df = df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today().strftime('%m_%d_%Y')\n",
    "file_location = 'https://raw.githubusercontent.com/saturncloud/flatiron-curriculum/main/links.csv'\n",
    "file_raw = requests.get(file_location)\n",
    "file_csv = StringIO(file_raw.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(file_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_path</th>\n",
       "      <th>consumer</th>\n",
       "      <th>enterprise</th>\n",
       "      <th>moringa</th>\n",
       "      <th>academyxi</th>\n",
       "      <th>vanguard</th>\n",
       "      <th>codeclan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phase1/dsc-PEP8/index.ipynb</td>\n",
       "      <td>https://app.flatironschool.saturnenterprise.io...</td>\n",
       "      <td>https://app.fisenterprise.saturnenterprise.io/...</td>\n",
       "      <td>https://app.moringa.saturnenterprise.io/dash/r...</td>\n",
       "      <td>https://app.academyxi.saturnenterprise.io/dash...</td>\n",
       "      <td>https://app.vanguarddigital.saturnenterprise.i...</td>\n",
       "      <td>https://app.codeclan.saturnenterprise.io/dash/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phase1/dsc-accessing-data-with-pandas-lab/inde...</td>\n",
       "      <td>https://app.flatironschool.saturnenterprise.io...</td>\n",
       "      <td>https://app.fisenterprise.saturnenterprise.io/...</td>\n",
       "      <td>https://app.moringa.saturnenterprise.io/dash/r...</td>\n",
       "      <td>https://app.academyxi.saturnenterprise.io/dash...</td>\n",
       "      <td>https://app.vanguarddigital.saturnenterprise.i...</td>\n",
       "      <td>https://app.codeclan.saturnenterprise.io/dash/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phase1/dsc-accessing-data-with-pandas/index.ipynb</td>\n",
       "      <td>https://app.flatironschool.saturnenterprise.io...</td>\n",
       "      <td>https://app.fisenterprise.saturnenterprise.io/...</td>\n",
       "      <td>https://app.moringa.saturnenterprise.io/dash/r...</td>\n",
       "      <td>https://app.academyxi.saturnenterprise.io/dash...</td>\n",
       "      <td>https://app.vanguarddigital.saturnenterprise.i...</td>\n",
       "      <td>https://app.codeclan.saturnenterprise.io/dash/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phase1/dsc-analyzing-macbeth-project-pandas-v2...</td>\n",
       "      <td>https://app.flatironschool.saturnenterprise.io...</td>\n",
       "      <td>https://app.fisenterprise.saturnenterprise.io/...</td>\n",
       "      <td>https://app.moringa.saturnenterprise.io/dash/r...</td>\n",
       "      <td>https://app.academyxi.saturnenterprise.io/dash...</td>\n",
       "      <td>https://app.vanguarddigital.saturnenterprise.i...</td>\n",
       "      <td>https://app.codeclan.saturnenterprise.io/dash/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phase1/dsc-combining-dataframes-pandas-lab/ind...</td>\n",
       "      <td>https://app.flatironschool.saturnenterprise.io...</td>\n",
       "      <td>https://app.fisenterprise.saturnenterprise.io/...</td>\n",
       "      <td>https://app.moringa.saturnenterprise.io/dash/r...</td>\n",
       "      <td>https://app.academyxi.saturnenterprise.io/dash...</td>\n",
       "      <td>https://app.vanguarddigital.saturnenterprise.i...</td>\n",
       "      <td>https://app.codeclan.saturnenterprise.io/dash/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          local_path  \\\n",
       "0                        Phase1/dsc-PEP8/index.ipynb   \n",
       "1  Phase1/dsc-accessing-data-with-pandas-lab/inde...   \n",
       "2  Phase1/dsc-accessing-data-with-pandas/index.ipynb   \n",
       "3  Phase1/dsc-analyzing-macbeth-project-pandas-v2...   \n",
       "4  Phase1/dsc-combining-dataframes-pandas-lab/ind...   \n",
       "\n",
       "                                            consumer  \\\n",
       "0  https://app.flatironschool.saturnenterprise.io...   \n",
       "1  https://app.flatironschool.saturnenterprise.io...   \n",
       "2  https://app.flatironschool.saturnenterprise.io...   \n",
       "3  https://app.flatironschool.saturnenterprise.io...   \n",
       "4  https://app.flatironschool.saturnenterprise.io...   \n",
       "\n",
       "                                          enterprise  \\\n",
       "0  https://app.fisenterprise.saturnenterprise.io/...   \n",
       "1  https://app.fisenterprise.saturnenterprise.io/...   \n",
       "2  https://app.fisenterprise.saturnenterprise.io/...   \n",
       "3  https://app.fisenterprise.saturnenterprise.io/...   \n",
       "4  https://app.fisenterprise.saturnenterprise.io/...   \n",
       "\n",
       "                                             moringa  \\\n",
       "0  https://app.moringa.saturnenterprise.io/dash/r...   \n",
       "1  https://app.moringa.saturnenterprise.io/dash/r...   \n",
       "2  https://app.moringa.saturnenterprise.io/dash/r...   \n",
       "3  https://app.moringa.saturnenterprise.io/dash/r...   \n",
       "4  https://app.moringa.saturnenterprise.io/dash/r...   \n",
       "\n",
       "                                           academyxi  \\\n",
       "0  https://app.academyxi.saturnenterprise.io/dash...   \n",
       "1  https://app.academyxi.saturnenterprise.io/dash...   \n",
       "2  https://app.academyxi.saturnenterprise.io/dash...   \n",
       "3  https://app.academyxi.saturnenterprise.io/dash...   \n",
       "4  https://app.academyxi.saturnenterprise.io/dash...   \n",
       "\n",
       "                                            vanguard  \\\n",
       "0  https://app.vanguarddigital.saturnenterprise.i...   \n",
       "1  https://app.vanguarddigital.saturnenterprise.i...   \n",
       "2  https://app.vanguarddigital.saturnenterprise.i...   \n",
       "3  https://app.vanguarddigital.saturnenterprise.i...   \n",
       "4  https://app.vanguarddigital.saturnenterprise.i...   \n",
       "\n",
       "                                            codeclan  \n",
       "0  https://app.codeclan.saturnenterprise.io/dash/...  \n",
       "1  https://app.codeclan.saturnenterprise.io/dash/...  \n",
       "2  https://app.codeclan.saturnenterprise.io/dash/...  \n",
       "3  https://app.codeclan.saturnenterprise.io/dash/...  \n",
       "4  https://app.codeclan.saturnenterprise.io/dash/...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = credentials.Credentials('C')\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {auth.API_KEY}'\n",
    "}\n",
    "course = canvas_interface.Course(auth.API_KEY, auth.API_PATH, 7)\n",
    "assignments = course.get_assignments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 632,\n",
       " 'description': '<header class=\"fis-header\" style=\"visibility: hidden;\"><a class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sklearn-preprocessing-lab\" target=\"_blank\"><img id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"></a><a class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sklearn-preprocessing-lab/issues/new\" target=\"_blank\"><img id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"></a></header>\\n<p>Work on this lab on your local computer. If you\\'re not sure how to clone the source code, refer to the instructions here: <a href=\"https://github.com/learn-co-curriculum/dsc-running-jupyter-locally\" target=\"_blank\">https://github.com/learn-co-curriculum/dsc-running-jupyter-locally</a></p>\\n<h3>Submission Instructions</h3>\\n<p>When you are finished with the lab, complete the following steps to submit your work:</p>\\n<ol>\\n<li>Save the changes to the notebook by clicking the Save icon, shown below highlighted in red<br><img src=\"https://codeclan.instructure.com/courses/7/files/130/preview?verifier=RVmWpKpaTUJlkZ5l93kLP3Mu80njyBHjLzZ73GhE\" alt=\"Screen Shot 2021-07-28 at 5.41.06 PM.png\" data-api-endpoint=\"https://codeclan.instructure.com/api/v1/courses/7/files/130\" data-api-returntype=\"File\">&nbsp;</li>\\n<li>Close the notebook browser tab(s)</li>\\n<li>Shut down the notebook server by typing control-C in the terminal window where it is currently running</li>\\n<li>Commit your changes in Git by typing <br><code>git commit -am \"Finished lab\"</code> <br>in the terminal and hitting Enter</li>\\n<li>Push your changes to GitHub by typing <br><code>git push origin master</code> <br>in the terminal and hitting Enter</li>\\n<li>Open the GitHub view of your fork of the lab in the browser. For example, if your username were <code>hoffm386</code>, you would go to <a href=\"https://github.com/hoffm386/dsc-data-serialization-lab\" target=\"_blank\">https://github.com/hoffm386/dsc-data-serialization-lab</a> in the browser for this particular lab. Click on <code>index.ipynb</code> and double-check that your code updates are there. (The updates will not be in the README, only in the <code>.ipynb</code> file.)</li>\\n<li>Submit the link to your fork of the lab in the textbox on Canvas<br>&nbsp;<img src=\"https://codeclan.instructure.com/courses/7/files/131/preview?verifier=2pBx87zo898bTwWJmKdJYLQA7QpzbTkVXIKpHzja\" alt=\"Screen Shot 2021-08-24 at 6.42.54 PM.png\" width=\"319\" height=\"320\" data-api-endpoint=\"https://codeclan.instructure.com/api/v1/courses/7/files/131\" data-api-returntype=\"File\"></li>\\n</ol>\\n<h3>Troubleshooting</h3>\\n<p><span style=\"background-color: #fbeeb8;\">If you are able to submit the URL successfully, you do not need to follow the below steps!</span></p>\\n<h4>Not a Git Repository</h4>\\n<p>If you try to run <code>git commit -am \"Finished lab\"</code> and get the error message <code>fatal: not a git repository</code>, double-check that you are running the code from the correct directory. If you type <code>pwd</code> in the terminal and hit Enter, the path that is printed out should include the directory of the lab — in this case, <code>dsc-data-serialization-lab</code>. For example, a valid path would be <code>/Users/myname/Development/DS/dsc-data-serialization-lab</code>, since that ends with the lab directory, whereas <code>/Users/myname/Development/DS/</code> would not be a valid path. Use commands like <code>ls</code> and <code>cd</code> to navigate to the appropriate directory, then continue with the steps above, starting with step 4.</p>\\n<h4>Permission Denied</h4>\\n<p>If you try to run <code>git push origin master</code> and get a <code>Permission denied</code> error message, you are likely trying to push to the curriculum version of the lab, not your personal fork. Follow these steps to fix this:</p>\\n<ol>\\n<li>In the browser, go to the curriculum repository for this lab by clicking the <img src=\"https://codeclan.instructure.com/courses/7/files/135/preview?verifier=tUVRazewiMH2bXaeYhDKoTMbY1dEv3UGwkb1Wame\" alt=\"GitHub octocat logo\" data-api-endpoint=\"https://codeclan.instructure.com/api/v1/courses/7/files/135\" data-api-returntype=\"File\">&nbsp;icon above</li>\\n<li>Click the Fork button. If you already have a fork, this will take you to it. If you haven\\'t made a fork yet, this will make the fork and take you to it</li>\\n<li>On the page of your fork, copy the clone link. For example, <a href=\"https://github.com/hoffm386/dsc-data-serialization-lab.git\" target=\"_blank\">https://github.com/hoffm386/dsc-data-serialization-lab.git</a></li>\\n<li>Back in the terminal where you were trying to run <code>git push</code>, type <br><code>git remote add myfork &lt;URL&gt;</code> <br>Where <code>&lt;URL&gt;</code> is replaced with the clone link you copied. For example, <code>git remote add myfork https://github.com/hoffm386/dsc-data-serialization-lab.git</code>. Then hit Enter. This means you have created a connection between your local repository and your fork</li>\\n<li>Now, push your code to your fork by typing <br><code>git push myfork master</code> <br>in the terminal and hitting Enter</li>\\n<li>Proceed with the steps above, starting with step 6</li>\\n</ol>',\n",
       " 'due_at': None,\n",
       " 'unlock_at': None,\n",
       " 'lock_at': None,\n",
       " 'points_possible': 1.0,\n",
       " 'grading_type': 'points',\n",
       " 'assignment_group_id': 68,\n",
       " 'grading_standard_id': None,\n",
       " 'created_at': '2023-07-18T15:20:15Z',\n",
       " 'updated_at': '2023-07-18T15:20:16Z',\n",
       " 'peer_reviews': False,\n",
       " 'automatic_peer_reviews': False,\n",
       " 'position': 1,\n",
       " 'grade_group_students_individually': False,\n",
       " 'anonymous_peer_reviews': False,\n",
       " 'group_category_id': None,\n",
       " 'post_to_sis': False,\n",
       " 'moderated_grading': False,\n",
       " 'omit_from_final_grade': False,\n",
       " 'intra_group_peer_reviews': False,\n",
       " 'anonymous_instructor_annotations': False,\n",
       " 'anonymous_grading': False,\n",
       " 'graders_anonymous_to_graders': False,\n",
       " 'grader_count': 0,\n",
       " 'grader_comments_visible_to_graders': True,\n",
       " 'final_grader_id': None,\n",
       " 'grader_names_visible_to_final_grader': True,\n",
       " 'allowed_attempts': -1,\n",
       " 'annotatable_attachment_id': None,\n",
       " 'hide_in_gradebook': False,\n",
       " 'secure_params': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJsdGlfYXNzaWdubWVudF9pZCI6IjQ5NDJmMzVkLTgwMDAtNGUzZi1iNjg4LWJhYjM0MTFkZTNmNyIsImx0aV9hc3NpZ25tZW50X2Rlc2NyaXB0aW9uIjoiXHUwMDNjaGVhZGVyIGNsYXNzPVwiZmlzLWhlYWRlclwiIHN0eWxlPVwidmlzaWJpbGl0eTogaGlkZGVuO1wiXHUwMDNlXHUwMDNjYSBjbGFzcz1cImZpcy1naXQtbGlua1wiIGhyZWY9XCJodHRwczovL2dpdGh1Yi5jb20vbGVhcm4tY28tY3VycmljdWx1bS9kc2Mtc2tsZWFybi1wcmVwcm9jZXNzaW5nLWxhYlwiIHRhcmdldD1cIl9ibGFua1wiXHUwMDNlXHUwMDNjaW1nIGlkPVwicmVwby1pbWdcIiB0aXRsZT1cIk9wZW4gR2l0SHViIFJlcG9cIiBhbHQ9XCJHaXRIdWIgUmVwb1wiXHUwMDNlXHUwMDNjL2FcdTAwM2VcdTAwM2NhIGNsYXNzPVwiZmlzLWdpdC1saW5rXCIgaHJlZj1cImh0dHBzOi8vZ2l0aHViLmNvbS9sZWFybi1jby1jdXJyaWN1bHVtL2RzYy1za2xlYXJuLXByZXByb2Nlc3NpbmctbGFiL2lzc3Vlcy9uZXdcIiB0YXJnZXQ9XCJfYmxhbmtcIlx1MDAzZVx1MDAzY2ltZyBpZD1cImlzc3VlLWltZ1wiIHRpdGxlPVwiQ3JlYXRlIE5ldyBJc3N1ZVwiIGFsdD1cIkNyZWF0ZSBOZXcgSXNzdWVcIlx1MDAzZVx1MDAzYy9hXHUwMDNlXHUwMDNjL2hlYWRlclx1MDAzZVxuXHUwMDNjcFx1MDAzZVdvcmsgb24gdGhpcyBsYWIgb24geW91ciBsb2NhbCBjb21wdXRlci4gSWYgeW91J3JlIG5vdCBzdXJlIGhvdyB0byBjbG9uZSB0aGUgc291cmNlIGNvZGUsIHJlZmVyIHRvIHRoZSBpbnN0cnVjdGlvbnMgaGVyZTogXHUwMDNjYSBocmVmPVwiaHR0cHM6Ly9naXRodWIuY29tL2xlYXJuLWNvLWN1cnJpY3VsdW0vZHNjLXJ1bm5pbmctanVweXRlci1sb2NhbGx5XCIgdGFyZ2V0PVwiX2JsYW5rXCJcdTAwM2VodHRwczovL2dpdGh1Yi5jb20vbGVhcm4tY28tY3VycmljdWx1bS9kc2MtcnVubmluZy1qdXB5dGVyLWxvY2FsbHlcdTAwM2MvYVx1MDAzZVx1MDAzYy9wXHUwMDNlXG5cdTAwM2NoM1x1MDAzZVN1Ym1pc3Npb24gSW5zdHJ1Y3Rpb25zXHUwMDNjL2gzXHUwMDNlXG5cdTAwM2NwXHUwMDNlV2hlbiB5b3UgYXJlIGZpbmlzaGVkIHdpdGggdGhlIGxhYiwgY29tcGxldGUgdGhlIGZvbGxvd2luZyBzdGVwcyB0byBzdWJtaXQgeW91ciB3b3JrOlx1MDAzYy9wXHUwMDNlXG5cdTAwM2NvbFx1MDAzZVxuXHUwMDNjbGlcdTAwM2VTYXZlIHRoZSBjaGFuZ2VzIHRvIHRoZSBub3RlYm9vayBieSBjbGlja2luZyB0aGUgU2F2ZSBpY29uLCBzaG93biBiZWxvdyBoaWdobGlnaHRlZCBpbiByZWRcdTAwM2Niclx1MDAzZVx1MDAzY2ltZyBzcmM9XCIvY291cnNlLi4uICh0cnVuY2F0ZWQpIn0.vRLa4LaClWZfSIrWCGKoEhvut-esqMWXD2hg-bb9Au4',\n",
       " 'lti_context_id': '4942f35d-8000-4e3f-b688-bab3411de3f7',\n",
       " 'course_id': 7,\n",
       " 'name': '⭐️ Preprocessing with scikit-learn - Cumulative Lab',\n",
       " 'submission_types': ['online_quiz'],\n",
       " 'has_submitted_submissions': False,\n",
       " 'due_date_required': False,\n",
       " 'max_name_length': 255,\n",
       " 'in_closed_grading_period': False,\n",
       " 'graded_submissions_exist': False,\n",
       " 'is_quiz_assignment': False,\n",
       " 'can_duplicate': False,\n",
       " 'original_course_id': None,\n",
       " 'original_assignment_id': None,\n",
       " 'original_lti_resource_link_id': None,\n",
       " 'original_assignment_name': None,\n",
       " 'original_quiz_id': None,\n",
       " 'workflow_state': 'published',\n",
       " 'important_dates': False,\n",
       " 'muted': True,\n",
       " 'html_url': 'https://codeclan.instructure.com/courses/7/assignments/632',\n",
       " 'has_overrides': False,\n",
       " 'needs_grading_count': 0,\n",
       " 'sis_assignment_id': None,\n",
       " 'integration_id': None,\n",
       " 'integration_data': {},\n",
       " 'quiz_id': 213,\n",
       " 'anonymous_submissions': False,\n",
       " 'published': True,\n",
       " 'unpublishable': True,\n",
       " 'only_visible_to_overrides': False,\n",
       " 'locked_for_user': False,\n",
       " 'submissions_download_url': 'https://codeclan.instructure.com/courses/7/quizzes/213/submissions?zip=1',\n",
       " 'post_manually': False,\n",
       " 'anonymize_students': False,\n",
       " 'require_lockdown_browser': False,\n",
       " 'restrict_quantitative_data': False}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course.assignments[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling HTML from dsc-ml-fundamentals-lab\n",
      "main pulled\n"
     ]
    }
   ],
   "source": [
    "lesson = lesson_content.HtmlBody(course.assignments[11], sc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h1>Machine Learning Fundamentals - Cumulative Lab</h1>\\n<h2>Introduction</h2>\\n<p>In this cumulative lab, you will work through an end-to-end machine learning workflow, focusing on the fundamental concepts of machine learning theory and processes. The main emphasis is on modeling theory (not EDA or preprocessing), so we will skip over some of the data visualization and data preparation steps that you would take in an actual modeling process.</p>\\n<h2>Objectives</h2>\\n<p>You will be able to:</p>\\n<ul>\\n<li>Recall the purpose of, and practice performing, a train-test split</li>\\n<li>Recall the difference between bias and variance</li>\\n<li>Practice identifying bias and variance in model performance</li>\\n<li>Practice applying strategies to minimize bias and variance</li>\\n<li>Practice selecting a final model and evaluating it on a holdout set</li>\\n</ul>\\n<h2>Your Task: Build a Model to Predict Blood Pressure</h2>\\n<p><img alt=\"stethoscope sitting on a case\" src=\"images/stethoscope.jpg\" /></p>\\n<p><span>Photo by <a href=\"https://unsplash.com/@marceloleal80?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Marcelo Leal</a> on <a href=\"https://unsplash.com/s/photos/blood-pressure?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span></p>\\n<h3>Business and Data Understanding</h3>\\n<p>Hypertension (high blood pressure) is a treatable condition, but measuring blood pressure requires specialized equipment that most people do not have at home.</p>\\n<p>The question, then, is <strong><em>can we predict blood pressure using just a scale and a tape measure</em></strong>? These measuring tools, which individuals are more likely to have at home, might be able to flag individuals with an increased risk of hypertension.</p>\\n<p><a href=\"https://doi.org/10.1155/2014/637635\">Researchers in Brazil</a> collected data from several hundred college students in order to answer this question. We will be specifically using the data they collected from female students.</p>\\n<p>The measurements we have are:</p>\\n<ul>\\n<li>Age (age in years)</li>\\n<li>BMI (body mass index, a ratio of weight to height)</li>\\n<li>WC (waist circumference in centimeters)</li>\\n<li>HC (hip circumference in centimeters)</li>\\n<li>WHR (waist-hip ratio)</li>\\n<li>SBP (systolic blood pressure)</li>\\n</ul>\\n<p>The chart below describes various blood pressure values:</p>\\n<p><a title=\"Ian Furst, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Hypertension_ranges_chart.png\"><img width=\"512\" alt=\"Hypertension ranges chart\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Hypertension_ranges_chart.png/512px-Hypertension_ranges_chart.png\"></a></p>\\n<h3>Requirements</h3>\\n<h4>1. Perform a Train-Test Split</h4>\\n<p>Load the data into a dataframe using pandas, separate the features (<code>X</code>) from the target (<code>y</code>), and use the <code>train_test_split</code> function to separate data into training and test sets.</p>\\n<h4>2. Build and Evaluate a First Simple Model</h4>\\n<p>Using the <code>LinearRegression</code> model and <code>mean_squared_error</code> function from scikit-learn, build and evaluate a simple linear regression model using the training data. Also, use <code>cross_val_score</code> to simulate unseen data, without actually using the holdout test set.</p>\\n<h4>3. Use <code>PolynomialFeatures</code> to Reduce Underfitting</h4>\\n<p>Apply a <code>PolynomialFeatures</code> transformer to give the model more ability to pick up on information from the training data. Test out different polynomial degrees until you have a model that is perfectly fit to the training data.</p>\\n<h4>4. Use Regularization to Reduce Overfitting</h4>\\n<p>Instead of a basic <code>LinearRegression</code>, use a <code>Ridge</code> regression model to apply regularization to the overfit model. In order to do this you will need to scale the data. Test out different regularization penalties to find the best model.</p>\\n<h4>5. Evaluate a Final Model on the Test Set</h4>\\n<p>Preprocess <code>X_test</code> and <code>y_test</code> appropriately in order to evaluate the performance of your final model on unseen data.</p>\\n<h2>1. Perform a Train-Test Split</h2>\\n<p>Before looking at the text below, try to remember: why is a train-test split the <em>first</em> step in a machine learning process?</p>\\n<hr />\\n<details>\\n    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\\n\\nA machine learning (predictive) workflow fundamentally emphasizes creating *a model that will perform well on unseen data*. We will hold out a subset of our original data as the \"test\" set that will stand in for truly unseen data that the model will encounter in the future.\\n\\nWe make this separation as the first step for two reasons:\\n\\n1. Most importantly, we are avoiding *leakage* of information from the test set into the training set. Leakage can lead to inflated metrics, since the model has information about the \"unseen\" data that it won\\'t have about real unseen data. This is why we always want to fit our transformers and models on the training data only, not the full dataset.\\n2. Also, we want to make sure the code we have written will actually work on unseen data. If we are able to transform our test data and evaluate it with our final model, that\\'s a good sign that the same process will work for future data as well.\\n\\n</details>\\n\\n<h3>Loading the Data</h3>\\n<p>In the cell below, we import the pandas library and open the full dataset for you. It has already been formatted and subsetted down to the relevant columns.</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>import pandas as pd\\ndf = pd.read_csv(\"data/blood_pressure.csv\", index_col=0)\\ndf\\n```</p>\\n<h3>Identifying Features and Target</h3>\\n<p>Once the data is loaded into a pandas dataframe, the next step is identifying which columns represent features and which column represents the target.</p>\\n<p>Recall that in this instance, we are trying to predict systolic blood pressure.</p>\\n<p>In the cell below, assign <code>X</code> to be the features and <code>y</code> to be the target. Remember that <code>X</code> should <strong>NOT</strong> contain the target.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<p>X = None\\ny = None</p>\\n<p>X\\n```</p>\\n<p>Make sure the assert statements pass before moving on to the next step:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<h1>X should be a 2D matrix with 224 rows and 5 columns</h1>\\n<p>assert X.shape == (224, 5)</p>\\n<h1>y should be a 1D array with 224 values</h1>\\n<p>assert y.shape == (224,)\\n```</p>\\n<h3>Performing Train-Test Split</h3>\\n<p>In the cell below, import <code>train_test_split</code> from scikit-learn (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">documentation here</a>).</p>\\n<p>Then create variables <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code> using <code>train_test_split</code> with <code>X</code>, <code>y</code>, and <code>random_state=2021</code>.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Import the relevant function</h1>\\n<p>None</p>\\n<h1>Create train and test data using random_state=2021</h1>\\n<p>None, None, None, None = None\\n```</p>\\n<p>Make sure that the assert statements pass:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert X_train.shape == (168, 5)\\nassert X_test.shape == (56, 5)</p>\\n<p>assert y_train.shape == (168,)\\nassert y_test.shape == (56,)\\n```</p>\\n<h2>2. Build and Evaluate a First Simple Model</h2>\\n<p>For our baseline model (FSM), we\\'ll use a <code>LinearRegression</code> from scikit-learn (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">documentation here</a>).</p>\\n<h3>Instantiating the Model</h3>\\n<p>In the cell below, instantiate a <code>LinearRegression</code> model and assign it to the variable <code>baseline_model</code>.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Import the relevant class</h1>\\n<p>None</p>\\n<h1>Instantiate a linear regression model</h1>\\n<p>baseline_model = None\\n```</p>\\n<p>Make sure the assert passes:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<h1>baseline_model should be a linear regression model</h1>\\n<p>assert type(baseline_model) == LinearRegression\\n```</p>\\n<p>If you are getting the type of <code>baseline_model</code> as <code>abc.ABCMeta</code>, make sure you actually invoked the constructor of the linear regression class with <code>()</code>.</p>\\n<p>If you are getting <code>NameError: name \\'LinearRegression\\' is not defined</code>, make sure you have the correct import statement.</p>\\n<h3>Fitting and Evaluating the Model on the Full Training Set</h3>\\n<p>In the cell below, fit the model on <code>X_train</code> and <code>y_train</code>:</p>\\n<p>```python</p>\\n<h1>Your code here</h1>\\n<p>```</p>\\n<p>Then, evaluate the model using root mean squared error (RMSE). To do this, first import the <code>mean_squared_error</code> function from scikit-learn (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\">documentation here</a>). Then pass in both the actual and predicted y values, along with <code>squared=False</code> (to get the RMSE rather than MSE).</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Import the relevant function</h1>\\n<p>None</p>\\n<h1>Generate predictions using baseline_model and X_train</h1>\\n<p>y_pred_baseline = None</p>\\n<h1>Evaluate using mean_squared_error with squared=False</h1>\\n<p>baseline_rmse = None\\nbaseline_rmse\\n```</p>\\n<p>Your RMSE calculation should be around 15.98:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert round(baseline_rmse, 2) == 15.98\\n```</p>\\n<p>This means that on the <em>training</em> data, our predictions are off by about 16 mmHg on average.</p>\\n<p>But what about on <em>unseen</em> data?</p>\\n<p>To stand in for true unseen data (and avoid making decisions based on this particular data split, therefore not using <code>X_test</code> or <code>y_test</code> yet), let\\'s use cross-validation.</p>\\n<h3>Fitting and Evaluating the Model with Cross Validation</h3>\\n<p>In the cell below, import <code>cross_val_score</code> (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\">documentation here</a>) and call it with <code>baseline_model</code>, <code>X_train</code>, and <code>y_train</code>.</p>\\n<p>For specific implementation reasons within the scikit-learn library, you\\'ll need to use <code>scoring=\"neg_root_mean_squared_error\"</code>, which returns the RMSE values with their signs flipped to negative. Then we take the average and negate it at the end, so the number is directly comparable to the RMSE number above.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Import the relevant function</h1>\\n<p>None</p>\\n<h1>Get the cross validated scores for our baseline model</h1>\\n<p>baseline_cv = None</p>\\n<h1>Display the average of the cross-validated scores</h1>\\n<p>baseline_cv_rmse = -(baseline_cv.mean())\\nbaseline_cv_rmse\\n```</p>\\n<p>The averaged RMSE for the cross-validated scores should be around 15.95:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert round(baseline_cv_rmse, 2) == 15.95\\n```</p>\\n<h3>Analysis of Baseline Model</h3>\\n<p>So, we got an RMSE of about 16 for both the training data and the validation data. RMSE is a form of <em>error</em>, so this means the performance is somewhat better on the validation data than the training data. (This is a bit unusual — normally we expect to see better scores on the training data, but maybe there are some outliers or other reasons that this particular split has this result.)</p>\\n<p>Referring back to the chart above, both errors mean that on average we would expect to mix up someone with stage 1 vs. stage 2 hypertension, but not someone with normal blood pressure vs. critical hypertension. So it appears that the features we have might be predictive enough to be useful.</p>\\n<p>Are we overfitting? Underfitting?</p>\\n<hr />\\n<details>\\n    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\\n\\nThe RMSE values for the training data and test data are fairly close to each other and the validation score is actually slightly better than the training score, so we can assume that we are not overfitting.\\n\\nIt seems like our model has some room for improvement, but without further investigation it\\'s impossible to know whether we are underfitting, or there is just irreducible error present. Maybe we are simply missing the features we would need to reduce error. (For example, we don\\'t know anything about the diets of these study participants, and we know that diet can influence blood pressure.) But it\\'s also possible that there is some reducible error, meaning we are currently underfitting.\\n\\nIn the next step, we\\'ll assume we *are* underfitting, and will attempt to reduce that underfitting by applying some polynomial features transformations to the data.\\n\\n</details>\\n\\n<h2>3. Use <code>PolynomialFeatures</code> to Reduce Underfitting</h2>\\n<p>Comprehension check: does \"underfitting\" mean we have high <em>bias</em>, or high <em>variance</em>?</p>\\n<hr />\\n<details>\\n    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\\n\\nUnderfitting means high bias. While it\\'s possible that your model will have both high bias and high variance at the same time, in general underfitting means that there is additional information in the data that your model currently isn\\'t picking up on, so you are getting higher error metrics than necessary.\\n\\n</details>\\n\\n<p>In some model algorithms (e.g. k-nearest neighbors) there are hyperparameters we can adjust so that the model is more flexible and can pick up on additional information in the data. In this case, since we are using linear regression, let\\'s instead perform some feature engineering with <code>PolynomialFeatures</code>.</p>\\n<h3>Creating <code>PolynomialFeatures</code> Transformer, Fitting and Transforming <code>X_train</code></h3>\\n<p>In the cell below, instantiate a <code>PolynomialFeatures</code> transformer with default arguments (i.e. just <code>PolynomialFeatures()</code>). Documentation for <code>PolynomialFeatures</code> can be found <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\">here</a>.</p>\\n<p>Then fit the transformer on <code>X_train</code> and create a new <code>X_train_poly</code> matrix by transforming <code>X_train</code>.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Import the relevant class</h1>\\n<p>None</p>\\n<h1>Instantiate polynomial features transformer</h1>\\n<p>poly = None</p>\\n<h1>Fit transformer on entire X_train</h1>\\n<p>None</p>\\n<h1>Create transformed data matrix by transforming X_train</h1>\\n<p>X_train_poly = None\\n```</p>\\n<p>Check that <code>poly</code> was instantiated correctly, and <code>X_train_poly</code> has the correct shape:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert type(poly) == PolynomialFeatures</p>\\n<p>assert X_train_poly.shape == (168, 21)\\n```</p>\\n<h3>Fitting and Evaluating the Model on the Transformed Training Set</h3>\\n<p>In the cell below, fit the <code>baseline_model</code> on <code>X_train_poly</code> and <code>y_train</code>, then find the RMSE using the same technique you used in Step 2.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Fit baseline_model</h1>\\n<p>None</p>\\n<h1>Make predictions</h1>\\n<p>y_pred_poly = None</p>\\n<h1>Find the RMSE on the full X_train_poly and y_train</h1>\\n<p>poly_rmse = None\\npoly_rmse\\n```</p>\\n<p>The new RMSE should be about 15.07:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert round(poly_rmse, 2) == 15.07\\n```</p>\\n<h3>Fitting and Evaluating the Model with Cross Validation</h3>\\n<p>In the cell below, use <code>cross_val_score</code> to find an averaged cross-validated RMSE using the same technique you used in Step 2.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Get the cross validated scores for our transformed features</h1>\\n<p>poly_cv = None</p>\\n<h1>Display the average of the cross-validated scores</h1>\\n<p>poly_cv_rmse = -(poly_cv.mean())\\npoly_cv_rmse\\n```</p>\\n<p>The cross-validated RMSE should be about 17.74:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert round(poly_cv_rmse, 2) == 17.74\\n```</p>\\n<h3>Analysis of <code>PolynomialFeatures</code> Transformation</h3>\\n<p>The cell below displays the baseline and transformed values for the full training set vs. the cross-validated average:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>print(\"Baseline Model\")\\nprint(\"Train RMSE:\", baseline_rmse)\\nprint(\"Validation RMSE:\", baseline_cv_rmse)\\nprint()\\nprint(\"Model with Polynomial Transformation\")\\nprint(\"Train RMSE:\", poly_rmse)\\nprint(\"Validation RMSE:\", poly_cv_rmse)\\n```</p>\\n<p>So, what does this mean about the result of our polynomial features transformation? What was the impact on bias (underfitting)? What was the impact on variance (overfitting)?</p>\\n<hr />\\n<details>\\n    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\\n\\nThe polynomial features transformation did successfully reduce bias (reduce underfitting). We can tell because the RMSE decreased on the training dataset. However, it also increased variance (increased overfitting). We can tell because the RMSE increased on the validation dataset compared to the train dataset.\\n\\nEssentially this means that the polynomial features transformation gave our model the ability to pick up on more information from the training dataset, but some of that information was actually \"noise\" and not information that was useful for making predictions on unseen data.\\n\\n</details>\\n\\n<p>In the cell below, we plot the train vs. validation RMSE across various different degrees of <code>PolynomialFeatures</code>:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<h1>Create lists of RMSE values</h1>\\n<p>train_rmse = []\\nval_rmse = []</p>\\n<h1>Create list of degrees we want to consider</h1>\\n<p>degrees = list(range(1,8))</p>\\n<p>for degree in degrees:\\n    # Create transformer of relevant degree and transform X_train\\n    poly = PolynomialFeatures(degree)\\n    X_train_poly = poly.fit_transform(X_train)\\n    baseline_model.fit(X_train_poly, y_train)</p>\\n<pre><code># RMSE for training data\\ny_pred_poly = baseline_model.predict(X_train_poly)\\ntrain_rmse.append(mean_squared_error(y_train, y_pred_poly, squared=False))\\n\\n# RMSE for validation data\\npoly_cv = cross_val_score(baseline_model, X_train_poly, y_train, scoring=\"neg_root_mean_squared_error\")\\nval_rmse.append(-(poly_cv.mean()))\\n</code></pre>\\n<h1>Set up plot</h1>\\n<p>import matplotlib.pyplot as plt\\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(13,5))</p>\\n<h1>Plot RMSE for training data</h1>\\n<p>ax1.plot(degrees, train_rmse)\\nax1.set_title(\"Training Data\")</p>\\n<h1>Plot RMSE for validation data</h1>\\n<p>ax2.plot(degrees, val_rmse, color=\"orange\")\\nax2.set_title(\"Validation Data\")</p>\\n<h1>Shared attributes for plots</h1>\\n<p>for ax in (ax1, ax2):\\n    ax.set_xticks(degrees)\\n    ax.set_xlabel(\"Polynomial Degree\")\\n    ax.set_ylabel(\"RMSE\")\\n```</p>\\n<p>Based on the above graphs, let\\'s plan to use a polynomial degree of 5. Why? Because that is where the RMSE for the training data has dropped down to essentially zero, meaning we are close to perfectly overfitting on the training data.</p>\\n<p>(This is a design decision where there isn\\'t always a single right answer. Later we will introduce a tool called \"grid search\" that will allow you to tune multiple aspects of the model at once instead of having to choose one step at a time like this.)</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<h1>Create transformer of relevant degree and transform X_train</h1>\\n<p>poly = PolynomialFeatures(5)\\nX_train_poly = poly.fit_transform(X_train)\\nbaseline_model.fit(X_train_poly, y_train)</p>\\n<h1>RMSE for training data</h1>\\n<p>y_pred_poly = baseline_model.predict(X_train_poly)\\nfinal_poly_rmse = mean_squared_error(y_train, y_pred_poly, squared=False)</p>\\n<h1>RMSE for validation data</h1>\\n<p>poly_cv = cross_val_score(baseline_model, X_train_poly, y_train, scoring=\"neg_root_mean_squared_error\")\\nfinal_poly_cv_rmse = -(poly_cv.mean())\\n```</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>print(\"Baseline Model\")\\nprint(\"Train RMSE:\", baseline_rmse)\\nprint(\"Validation RMSE:\", baseline_cv_rmse)\\nprint()\\nprint(\"Model with Polynomial Transformation (Degree 5)\")\\nprint(\"Train RMSE:\", final_poly_rmse)\\nprint(\"Validation RMSE:\", final_poly_cv_rmse)\\n```</p>\\n<p>We have a dramatically improved train RMSE (approximately 16 down to 0) and a dramatically worsened validation RMSE (approximately 16 up to 17,000). At this point we are clearly overfitting, but we have successfully reduced the underfitting on the training dataset.</p>\\n<p>In the next step, let\\'s apply a technique to address this overfitting.</p>\\n<h2>4. Use Regularization to Reduce Overfitting</h2>\\n<p>Let\\'s use regularization to address this overfitting, specifically using the <code>Ridge</code> model from scikit-learn (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">documentation here</a>), which uses the L2 norm.</p>\\n<h3>Scaling the Data</h3>\\n<p>Because L2 regularization is distance-based, we need to scale our data before passing it into this model. In the cell below, instantiate a <code>StandardScaler</code> (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">documentation here</a>) and fit then transform the full <code>X_train_poly</code>.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Import the relevant class</h1>\\n<p>None</p>\\n<h1>Instantiate the scaler</h1>\\n<p>scaler = None</p>\\n<h1>Fit the scaler on X_train_poly</h1>\\n<p>None</p>\\n<h1>Transform the data and create a new matrix</h1>\\n<p>X_train_scaled = None\\n```</p>\\n<p>The scaled data should have the same shape as <code>X_train_poly</code> but the values should be different:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert X_train_scaled.shape == X_train_poly.shape\\nassert X_train_scaled[0][0] != X_train_poly[0][0]\\n```</p>\\n<h3>Fitting a Ridge Model</h3>\\n<p>In the cell below, instantiate a <code>Ridge</code> model with <code>random_state=42</code>, then fit it on <code>X_train_scaled</code> and <code>y_train</code>.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Import the relevant class</h1>\\n<p>None</p>\\n<h1>Instantiate the model with random_state=42</h1>\\n<p>ridge_model = None</p>\\n<h1>Fit the model</h1>\\n<p>None\\n```</p>\\n<h3>Metrics for Ridge Model</h3>\\n<p>Now, find the train and cross-validated RMSE values, and assign them to <code>ridge_rmse</code> and <code>ridge_cv_rmse</code> respectively. You can refer back to previous steps to remember how to do this! Remember to use <code>ridge_model</code> and <code>X_train_scaled</code>.</p>\\n<p>```python</p>\\n<h1>Your code here</h1>\\n<p>print(\"Train RMSE:\", ridge_rmse)\\nprint(\"Validation RMSE:\", ridge_cv_rmse)\\n```</p>\\n<p>Your train RMSE should be about 15.24, and validation RMSE should be about 16.05:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert round(ridge_rmse, 2) == 15.24\\nassert round(ridge_cv_rmse, 2) == 16.05\\n```</p>\\n<h3>Analysis of Model with Regularization</h3>\\n<p>The following cell shows metrics for each model so far:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>print(\"Baseline Model\")\\nprint(\"Train RMSE:\", baseline_rmse)\\nprint(\"Validation RMSE:\", baseline_cv_rmse)\\nprint()\\nprint(\"Model with Polynomial Transformation (Degree 5)\")\\nprint(\"Train RMSE:\", final_poly_rmse)\\nprint(\"Validation RMSE:\", final_poly_cv_rmse)\\nprint()\\nprint(\"Model with Polynomial Transformation + Regularization\")\\nprint(\"Train RMSE:\", ridge_rmse)\\nprint(\"Validation RMSE:\", ridge_cv_rmse)\\n```</p>\\n<p>Did we successfully reduce overfitting? Which model is the best model so far?</p>\\n<hr />\\n<details>\\n    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\\n\\nCompared to the model with the polynomial transformation, yes, we successfully reduced overfitting. We can tell because the gap between the train and validation RMSE got a lot smaller.\\n\\nAt this point, our best model is actually still the baseline model. Even though we have a lower RMSE for the training data with both the model with polynomial transformation and the model with regularization added, the validation RMSE was still lowest for the baseline model.\\n\\n</details>\\n\\n<p>Let\\'s try adding stronger regularization penalties, to see if we can reduce the overfitting a bit further while still keeping the improvements to underfitting that we got from the polynomial features transformation.</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<h1>Create lists of RMSE values</h1>\\n<p>train_rmse = []\\nval_rmse = []</p>\\n<h1>Create list of alphas we want to consider</h1>\\n<p>alphas = [1, 10, 25, 50, 75, 100, 125, 250, 500]</p>\\n<p>for alpha in alphas:\\n    # Fit a model with a given regularization penalty\\n    model = Ridge(random_state=42, alpha=alpha)\\n    model.fit(X_train_scaled, y_train)</p>\\n<pre><code># RMSE for training data\\ny_pred_ridge = model.predict(X_train_scaled)\\ntrain_rmse.append(mean_squared_error(y_train, y_pred_ridge, squared=False))\\n\\n# RMSE for validation data\\nridge_cv = cross_val_score(model, X_train_scaled, y_train, scoring=\"neg_root_mean_squared_error\")\\nval_rmse.append(-(ridge_cv.mean()))\\n</code></pre>\\n<h1>Plot train vs. validation RMSE</h1>\\n<p>fig, ax = plt.subplots(figsize=(6,6))\\nax.plot(alphas, train_rmse, label=\"Training Data\")\\nax.plot(alphas, val_rmse, label=\"Validation Data\")\\nax.set_xlabel(\"Alpha (Regularization Penalty)\")\\nax.set_ylabel(\"RMSE\")\\nax.legend();\\n```</p>\\n<p>(This time both are plotted on the same axes because the RMSE has the same order of magnitude.)</p>\\n<p>As we increase the alpha (regularization penalty) along the x-axis, first we can see a big drop in the validation RMSE, then as we keep penalizing more, eventually the RMSE for both the training and validation data starts increasing (meaning we are starting to underfit again).</p>\\n<p>The code below finds the best alpha value from our list, i.e. the alpha that results in the lowest RMSE for the validation data:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>lowest_rmse = min(val_rmse)\\nprint(\"Lowest RMSE:\", lowest_rmse)</p>\\n<p>best_alpha = alphas[val_rmse.index(lowest_rmse)]\\nprint(\"Best alpha:\", best_alpha)\\n```</p>\\n<p>Let\\'s build a final model using that alpha value and compare it to our previous models:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<h1>Fit a model with a given regularization penalty</h1>\\n<p>final_model = Ridge(random_state=42, alpha=best_alpha)\\nfinal_model.fit(X_train_scaled, y_train)</p>\\n<h1>RMSE for training data</h1>\\n<p>y_pred_final = final_model.predict(X_train_scaled)\\nfinal_rmse = mean_squared_error(y_train, y_pred_final, squared=False)</p>\\n<h1>RMSE for validation data</h1>\\n<p>final_cv = cross_val_score(final_model, X_train_scaled, y_train, scoring=\"neg_root_mean_squared_error\")\\nfinal_cv_rmse = -(final_cv.mean())\\n```</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>print(\"Baseline Model\")\\nprint(\"Train RMSE:\", baseline_rmse)\\nprint(\"Validation RMSE:\", baseline_cv_rmse)\\nprint()\\nprint(\"Model with Polynomial Transformation (Degree 5)\")\\nprint(\"Train RMSE:\", final_poly_rmse)\\nprint(\"Validation RMSE:\", final_poly_cv_rmse)\\nprint()\\nprint(\"Final Model with Polynomial Transformation + Regularization\")\\nprint(\"Train RMSE:\", final_rmse)\\nprint(\"Validation RMSE:\", final_cv_rmse)\\n```</p>\\n<h3>Choosing a Final Model</h3>\\n<p>While we have already labeled a model as <code>final_model</code> above, make sure you understand why: this is the model with the best (lowest) validation RMSE. We also improved the train RMSE somewhat as well, meaning that our modeling strategy has actually reduced both underfitting and overfitting!</p>\\n<p>The impact of the changes made so far has been minimal, which makes sense given our business context. We are trying to predict blood pressure based on proxy measurements that leave out a lot of important information! But we still did see some improvement over the baseline by applying polynomial feature transformation and regularization.</p>\\n<h2>5. Evaluate a Final Model on the Test Set</h2>\\n<p>Often our lessons leave out this step because we are focused on other concepts, but if you were to present your final model to stakeholders, it\\'s important to perform one final analysis on truly unseen data to make sure you have a clear idea of how the model will perform in the field.</p>\\n<h3>Instantiating the Final Model</h3>\\n<p>Unless you are using a model that is very slow to fit, it\\'s a good idea to re-create it from scratch prior to the final evaluation. That way you avoid any artifacts of how you iterated on the model previously.</p>\\n<p>In the cell below, instantiate a <code>Ridge</code> model with <code>random_state=42</code> and <code>alpha=100</code>.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<p>final_model = None\\n```</p>\\n<h3>Fitting the Final Model on the Training Data</h3>\\n<p>You can go ahead and use the <code>X_train_scaled</code> and <code>y_train</code> data we created earlier.</p>\\n<p>```python</p>\\n<h1>Your code here</h1>\\n<p>```</p>\\n<h3>Preprocessing the Test Set</h3>\\n<p>The training data for our final model was transformed in two ways:</p>\\n<ol>\\n<li>Polynomial features added by the <code>poly</code> transformer object</li>\\n<li>Scaled by the <code>scaler</code> transformer object</li>\\n</ol>\\n<p>In the cell below, transform the test data in the same way, with the same transformer objects. Do NOT re-instantiate or re-fit these objects.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Add polynomial features</h1>\\n<p>X_test_poly = None</p>\\n<h1>Scale data</h1>\\n<p>X_test_scaled = None\\n```</p>\\n<p>Make sure the shape is correct. If you have too few columns, make sure that you passed the transformed version of <code>X_test</code> (<code>X_test_poly</code>) to the scaler rather than just <code>X_test</code>.</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>assert X_test_scaled.shape == (56, 252)\\n```</p>\\n<h3>Evaluating RMSE with Final Model and Preprocessed Test Set</h3>\\n<p>This time we don\\'t need to use cross-validation, since we are using the test set. In the cell below, generate predictions for the test data then use <code>mean_squared_error</code> with <code>squared=False</code> to find the RMSE for our holdout test set.</p>\\n<p>```python</p>\\n<h1>Replace None with appropriate code</h1>\\n<h1>Generate predictions</h1>\\n<p>y_pred_test = None</p>\\n<h1>Find RMSE</h1>\\n<p>test_rmse = None\\ntest_rmse\\n```</p>\\n<h3>Interpreting Our Results</h3>\\n<p>So, we successfully used polynomial features transformation and regularization to improve our metrics. But, can we recommend that this model be used for the purpose of predicting blood pressure based on these features?</p>\\n<p>Let\\'s create a scatter plot of actual vs. predicted blood pressure, with the boundaries of high blood pressure indicated:</p>\\n<p>```python</p>\\n<h1>Run this cell without changes</h1>\\n<p>import seaborn as sns</p>\\n<h1>Set up plot</h1>\\n<p>fig, ax = plt.subplots(figsize=(8,6))</p>\\n<h1>Seaborn scatter plot with best fit line</h1>\\n<p>sns.regplot(x=y_test, y=y_pred_test, ci=None, truncate=False, ax=ax)\\nax.set_xlabel(\"Actual Blood Pressure\")\\nax.set_ylabel(\"Predicted Blood Pressure\")</p>\\n<h1>Add spans showing high blood pressure + legend</h1>\\n<p>ax.axvspan(129, max(y_test) + 1, alpha=0.2, color=\"blue\", label=\"actual high blood pressure risk\")\\nax.axhspan(129, max(y_pred_test) + 1, alpha=0.2, color=\"gray\", label=\"predicted high blood pressure risk\")\\nax.legend();\\n```</p>\\n<p>In general, as the true blood pressure values increase, so do the predicted blood pressure values. So, it\\'s clear that our model is picking up on <em>some</em> information from our features.</p>\\n<p>But it looks like this model does not actually solve the initial business problem very well. Recall that our question was: <strong><em>can we predict blood pressure using just a scale and a tape measure?</em></strong> Our model would incorrectly flag one person as being at risk of high blood pressure, while missing all of the people who actually are at risk of high blood pressure.</p>\\n<p>It is possible that some other model algorithm (e.g. k-nearest neighbors or decision trees) would do a better job of picking up on the underlying patterns in this dataset. Or if we set this up as a classification problem rather than a regression problem, if we\\'re only interested in flagging high blood pressure rather than predicting blood pressure in general.</p>\\n<p>But if we had to stop this analysis now in its current state, we would need to conclude that <strong>while we were able to pick up some information about blood pressure using these variables alone, we did not produce a model that would work for this business case</strong>.</p>\\n<p>This is something that happens sometimes — not every target can be predicted with the features you have been given! In this case, maybe your model would still be useful for epidemiological modeling (predicting the blood pressure in populations) rather than predicting blood pressure for an individual, since we are picking up on some information. Further study would be needed to determine the feasibility of this approach.</p>\\n<h2>Summary</h2>\\n<p>In this cumulative lab, you performed an end-to-end machine learning process with correct usage of training, validation, and test data. You identified underfitting and overfitting and applied strategies to address them. Finally, you evaluated your final model using test data, and interpreted those results in the context of a business problem.</p>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesson.markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h2>Introduction</h2>\\n<p>In this cumulative lab, you will work through an end-to-end machine learning workflow, focusing on the fundamental concepts of machine learning theory and processes. The main emphasis is on modeling theory (not EDA or preprocessing), so we will skip over some of the data visualization and data preparation steps that you would take in an actual modeling process.</p>\\n<h2>Objectives</h2>\\n<p>You will be able to:</p>\\n<ul>\\n<li>Recall the purpose of, and practice performing, a train-test split</li>\\n<li>Recall the difference between bias and variance</li>\\n<li>Practice identifying bias and variance in model performance</li>\\n<li>Practice applying strategies to minimize bias and variance</li>\\n<li>Practice selecting a final model and evaluating it on a holdout set</li>\\n<'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesson_content.get_intro(lesson.markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = credentials.Credentials('C')\n",
    "headers = {\n",
    "        'Authorization': f'Bearer {auth.API_KEY}'\n",
    "    }\n",
    "\n",
    "done = False\n",
    "page = 1\n",
    "assignments = []\n",
    "while(not done):\n",
    "    assn_url = f\"{auth.API_PATH}/courses/7/assignments?per_page=100&page={page}\"\n",
    "    assn_response = requests.get(assn_url, headers=headers)\n",
    "    response_list = assn_response.json()\n",
    "    assignments.extend(response_list)\n",
    "    if (len(response_list) < 100):\n",
    "        done = True\n",
    "    else:\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 714,\n",
       " 'description': '<div id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-canvas-pdf-instructions\"></div>\\n<header class=\"fis-header\" style=\"visibility: hidden;\"><a class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-canvas-pdf-instructions\" target=\"_blank\"><img id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"></a><a class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-canvas-pdf-instructions/issues/new\" target=\"_blank\"><img id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"></a></header>\\n<p>To submit your project in Canvas, you will create and upload PDF versions of three project deliverables.</p>\\n<p>This page contains instructions for creating these PDFs, as well as how to submit them on Canvas.</p>\\n<h2>PDF Creation Instructions</h2>\\n<h3>Presentation Slides PDF Creation</h3>\\n<ol>\\n<li>Export your presentation as a PDF from the program in which you created it.</li>\\n<li>Locate the file in your file system, and rename the file to <code>presentation.pdf</code>.</li>\\n<li>Place a copy of the PDF in your GitHub repository. (Make sure you add, commit, and push this change.)</li>\\n</ol>\\n<h3>GitHub Repository PDF Creation</h3>\\n<ol>\\n<li>Navigate to the root directory of your project repository on GitHub, using your browser (we recommend Google Chrome).</li>\\n<li>Save the webpage as a PDF using the browser\\'s Print functionality (<a href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\">Google Chrome Save to PDF instructions</a>)</li>\\n<li>Locate the file in your file system, and give it a short descriptive file name (e.g. <code>github.pdf</code>).</li>\\n</ol>\\n<p><img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-canvas-pdf-instructions/master/repo_pdf.gif\" alt=\"Repository PDF Creation\"></p>\\n<h3>Jupyter Notebook PDF Creation</h3>\\n<ol>\\n<li>Open your Notebook in your browser (we recommend Google Chrome).</li>\\n<li>\\n<strong>Run the Notebook from start to finish</strong> so that your output is visible.</li>\\n<li>Save the page as a PDF using the browser\\'s Print functionality (<a href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\">Google Chrome Save to PDF instructions</a>)</li>\\n<li>Locate the file in your file system, and give it a short descriptive file name (e.g. <code>notebook.pdf</code>).</li>\\n</ol>\\n<p>If you have difficulty creating a PDF version of your notebook, you can use <a href=\"https://htmtopdf.herokuapp.com/ipynbviewer/\">this tool</a> instead. Set the Results Format to HTML + PDF. Then click View and Convert. Once its done, you should see links to .html and .pdf versions above the View and Convert button.</p>\\n<p>Another alternative, which can create a professional-looking report-style PDF, is to use the <code>nbconvert</code> tool (<a href=\"https://nbconvert.readthedocs.io/en/latest/\">documentation here</a>). This option is more complicated to use compared to the in-browser options.</p>\\n<h2>PDF Submission in Canvas</h2>\\n<p>You will need to submit all three PDF files as a single submission:</p>\\n<ol>\\n<li>Click \"Submit Assignment\" at the top of this page (the \"Phase X Project - PDFs\" assignment in the \"Milestones\" topic).</li>\\n<li>In the \"File Upload\" box, click \"Choose File\" button to upload a single file.</li>\\n<li>Click the \"Add Another File\" link to upload an additional file.</li>\\n<li>Repeat Step 3 to upload one more file. After this is done, all three files should be uploaded.</li>\\n<li>Hit the blue \"Submit Assignment\" button.</li>\\n</ol>\\n<p><img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-canvas-pdf-instructions/master/project_3pdf_submission.gif\" alt=\"Project PDF Submission\"></p>',\n",
       " 'due_at': None,\n",
       " 'unlock_at': None,\n",
       " 'lock_at': None,\n",
       " 'points_possible': 16.0,\n",
       " 'grading_type': 'gpa_scale',\n",
       " 'assignment_group_id': 71,\n",
       " 'grading_standard_id': None,\n",
       " 'created_at': '2023-07-18T15:20:47Z',\n",
       " 'updated_at': '2023-07-18T15:20:48Z',\n",
       " 'peer_reviews': False,\n",
       " 'automatic_peer_reviews': False,\n",
       " 'position': 10,\n",
       " 'grade_group_students_individually': False,\n",
       " 'anonymous_peer_reviews': False,\n",
       " 'group_category_id': None,\n",
       " 'post_to_sis': False,\n",
       " 'moderated_grading': False,\n",
       " 'omit_from_final_grade': False,\n",
       " 'intra_group_peer_reviews': False,\n",
       " 'anonymous_instructor_annotations': False,\n",
       " 'anonymous_grading': False,\n",
       " 'graders_anonymous_to_graders': False,\n",
       " 'grader_count': 0,\n",
       " 'grader_comments_visible_to_graders': True,\n",
       " 'final_grader_id': None,\n",
       " 'grader_names_visible_to_final_grader': True,\n",
       " 'allowed_attempts': -1,\n",
       " 'annotatable_attachment_id': None,\n",
       " 'hide_in_gradebook': False,\n",
       " 'secure_params': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJsdGlfYXNzaWdubWVudF9pZCI6IjRhNGQyMTZmLTI1ZmUtNDgxNy04MzRiLTIwMDA4YmVlMmVkNyIsImx0aV9hc3NpZ25tZW50X2Rlc2NyaXB0aW9uIjoiXHUwMDNjZGl2IGlkPVwiZ2l0LWRhdGEtZWxlbWVudFwiIGRhdGEtb3JnPVwibGVhcm4tY28tY3VycmljdWx1bVwiIGRhdGEtcmVwbz1cImRzYy1jYW52YXMtcGRmLWluc3RydWN0aW9uc1wiXHUwMDNlXHUwMDNjL2Rpdlx1MDAzZVxuXHUwMDNjaGVhZGVyIGNsYXNzPVwiZmlzLWhlYWRlclwiIHN0eWxlPVwidmlzaWJpbGl0eTogaGlkZGVuO1wiXHUwMDNlXHUwMDNjYSBjbGFzcz1cImZpcy1naXQtbGlua1wiIGhyZWY9XCJodHRwczovL2dpdGh1Yi5jb20vbGVhcm4tY28tY3VycmljdWx1bS9kc2MtY2FudmFzLXBkZi1pbnN0cnVjdGlvbnNcIiB0YXJnZXQ9XCJfYmxhbmtcIlx1MDAzZVx1MDAzY2ltZyBpZD1cInJlcG8taW1nXCIgdGl0bGU9XCJPcGVuIEdpdEh1YiBSZXBvXCIgYWx0PVwiR2l0SHViIFJlcG9cIlx1MDAzZVx1MDAzYy9hXHUwMDNlXHUwMDNjYSBjbGFzcz1cImZpcy1naXQtbGlua1wiIGhyZWY9XCJodHRwczovL2dpdGh1Yi5jb20vbGVhcm4tY28tY3VycmljdWx1bS9kc2MtY2FudmFzLXBkZi1pbnN0cnVjdGlvbnMvaXNzdWVzL25ld1wiIHRhcmdldD1cIl9ibGFua1wiXHUwMDNlXHUwMDNjaW1nIGlkPVwiaXNzdWUtaW1nXCIgdGl0bGU9XCJDcmVhdGUgTmV3IElzc3VlXCIgYWx0PVwiQ3JlYXRlIE5ldyBJc3N1ZVwiXHUwMDNlXHUwMDNjL2FcdTAwM2VcdTAwM2MvaGVhZGVyXHUwMDNlXG5cdTAwM2NwXHUwMDNlVG8gc3VibWl0IHlvdXIgcHJvamVjdCBpbiBDYW52YXMsIHlvdSB3aWxsIGNyZWF0ZSBhbmQgdXBsb2FkIFBERiB2ZXJzaW9ucyBvZiB0aHJlZSBwcm9qZWN0IGRlbGl2ZXJhYmxlcy5cdTAwM2MvcFx1MDAzZVxuXHUwMDNjcFx1MDAzZVRoaXMgcGFnZSBjb250YWlucyBpbnN0cnVjdGlvbnMgZm9yIGNyZWF0aW5nIHRoZXNlIFBERnMsIGFzIHdlbGwgYXMgaG93IHRvIHN1Ym1pdCB0aGVtIG9uIENhbnZhcy5cdTAwM2MvcFx1MDAzZVxuXHUwMDNjaDJcdTAwM2VQREYgQ3JlYXRpb24gSW5zdHJ1Y3Rpb25zXHUwMDNjL2gyXHUwMDNlXG5cdTAwM2NoM1x1MDAzZVByZXNlbnRhdGlvbiBTbGlkZXMgUERGIENyZWF0aW9uXHUwMDNjL2gzXHUwMDNlXG5cdTAwM2NvbFx1MDAzZVxuXHUwMDNjbGlcdTAwM2VFeHBvcnQgeW91ciBwcmVzZW50YXRpb24gYXMgYSBQREYgZnJvbSB0aGUgcHJvZ3JhbSBpbiB3aGljaCB5b3UgY3JlYXRlZCBpdC5cdTAwM2MvbGlcdTAwM2Vcblx1MDAzY2xpXHUwMDNlTG9jYXRlIHRoZSBmaWxlIGluIHlvdXIgZmlsZSBzeXN0ZW0sIGFuZCByZW5hbWUgdGguLi4gKHRydW5jYXRlZCkifQ.FtR86ak_JlKkz_N8lJv8kiSBFrXiMgLPjoOEfMe7UX0',\n",
       " 'lti_context_id': '4a4d216f-25fe-4817-834b-20008bee2ed7',\n",
       " 'course_id': 7,\n",
       " 'name': 'Phase 3 Project - PDFs',\n",
       " 'submission_types': ['online_upload'],\n",
       " 'has_submitted_submissions': False,\n",
       " 'due_date_required': False,\n",
       " 'max_name_length': 255,\n",
       " 'in_closed_grading_period': False,\n",
       " 'graded_submissions_exist': False,\n",
       " 'is_quiz_assignment': False,\n",
       " 'can_duplicate': True,\n",
       " 'original_course_id': None,\n",
       " 'original_assignment_id': None,\n",
       " 'original_lti_resource_link_id': None,\n",
       " 'original_assignment_name': None,\n",
       " 'original_quiz_id': None,\n",
       " 'workflow_state': 'unpublished',\n",
       " 'important_dates': False,\n",
       " 'muted': True,\n",
       " 'html_url': 'https://codeclan.instructure.com/courses/7/assignments/714',\n",
       " 'has_overrides': False,\n",
       " 'needs_grading_count': 0,\n",
       " 'sis_assignment_id': None,\n",
       " 'integration_id': None,\n",
       " 'integration_data': {},\n",
       " 'allowed_extensions': ['pdf'],\n",
       " 'use_rubric_for_grading': False,\n",
       " 'free_form_criterion_comments': False,\n",
       " 'rubric': [{'id': '_7005',\n",
       "   'points': 4.0,\n",
       "   'description': 'Attention to Detail',\n",
       "   'long_description': None,\n",
       "   'ignore_for_scoring': None,\n",
       "   'criterion_use_range': False,\n",
       "   'ratings': [{'id': 'blank',\n",
       "     'points': 4.0,\n",
       "     'description': 'Exceeds Objective',\n",
       "     'long_description': '90% or more of the project checklist items are complete'},\n",
       "    {'id': '_572',\n",
       "     'points': 3.0,\n",
       "     'description': 'Meets Objective',\n",
       "     'long_description': '80% or more of the project checklist items are complete'},\n",
       "    {'id': '_1169',\n",
       "     'points': 2.0,\n",
       "     'description': 'Approaching Objective',\n",
       "     'long_description': '70% or more of the project checklist items are complete'},\n",
       "    {'id': 'blank_2',\n",
       "     'points': 1.0,\n",
       "     'description': 'Does Not Meet Objective',\n",
       "     'long_description': '60% or fewer of the project checklist items are complete'}]},\n",
       "  {'id': '_9858',\n",
       "   'points': 4.0,\n",
       "   'description': 'ML Communication',\n",
       "   'long_description': None,\n",
       "   'ignore_for_scoring': None,\n",
       "   'criterion_use_range': False,\n",
       "   'ratings': [{'id': '_5934',\n",
       "     'points': 4.0,\n",
       "     'description': 'Exceeds Objective',\n",
       "     'long_description': 'Communicates the rationale, results, limitations, and specific recommendations generated by a classification model'},\n",
       "    {'id': '_2453',\n",
       "     'points': 3.0,\n",
       "     'description': 'Meets Objective',\n",
       "     'long_description': 'Successfully communicates model metrics without any major errors'},\n",
       "    {'id': '_2259',\n",
       "     'points': 2.0,\n",
       "     'description': 'Approaching Objective',\n",
       "     'long_description': 'Communicates model metrics with at least one major error'},\n",
       "    {'id': '_7065',\n",
       "     'points': 1.0,\n",
       "     'description': 'Does Not Meet Objective',\n",
       "     'long_description': 'Does not communicate model metrics'}]},\n",
       "  {'id': '_4427',\n",
       "   'points': 4.0,\n",
       "   'description': 'Data Preparation for Machine Learning',\n",
       "   'long_description': None,\n",
       "   'ignore_for_scoring': None,\n",
       "   'criterion_use_range': False,\n",
       "   'ratings': [{'id': '_4301',\n",
       "     'points': 4.0,\n",
       "     'description': 'Exceeds Objective',\n",
       "     'long_description': 'Goes above and beyond with data preparation, such as feature engineering or using pipelines'},\n",
       "    {'id': '_1679',\n",
       "     'points': 3.0,\n",
       "     'description': 'Meets Objective',\n",
       "     'long_description': 'Successfully prepares data for modeling, using a final holdout dataset that is transformed by (but not fitted on) transformers used to prepare training data AND scaling data when appropriate'},\n",
       "    {'id': '_591',\n",
       "     'points': 2.0,\n",
       "     'description': 'Approaching Objective',\n",
       "     'long_description': 'Prepares some data successfully, but has at least one major error'},\n",
       "    {'id': '_2569',\n",
       "     'points': 1.0,\n",
       "     'description': 'Does Not Meet Objective',\n",
       "     'long_description': 'Does not prepare data for modeling'}]},\n",
       "  {'id': '_5003',\n",
       "   'points': 4.0,\n",
       "   'description': 'Nonparametric and Ensemble Modeling',\n",
       "   'long_description': None,\n",
       "   'ignore_for_scoring': None,\n",
       "   'criterion_use_range': False,\n",
       "   'ratings': [{'id': '_7507',\n",
       "     'points': 4.0,\n",
       "     'description': 'Exceeds Objective',\n",
       "     'long_description': 'Goes above and beyond in the modeling process, such as articulating why a given model type is best suited to the problem or correctly using scikit-learn models not covered in the curriculum'},\n",
       "    {'id': '_1398',\n",
       "     'points': 3.0,\n",
       "     'description': 'Meets Objective',\n",
       "     'long_description': 'Uses at least two types of scikit-learn model and tunes at least one hyperparameter in a justifiable way without any major errors'},\n",
       "    {'id': '_2036',\n",
       "     'points': 2.0,\n",
       "     'description': 'Approaching Objective',\n",
       "     'long_description': 'Builds multiple classification models with at least one major error'},\n",
       "    {'id': '_8736',\n",
       "     'points': 1.0,\n",
       "     'description': 'Does Not Meet Objective',\n",
       "     'long_description': 'Does not build multiple classification models'}]}],\n",
       " 'rubric_settings': {'id': 25,\n",
       "  'title': 'Phase 3 Project - v2.3',\n",
       "  'points_possible': 16.0,\n",
       "  'free_form_criterion_comments': False,\n",
       "  'hide_score_total': False,\n",
       "  'hide_points': False},\n",
       " 'published': False,\n",
       " 'unpublishable': True,\n",
       " 'only_visible_to_overrides': False,\n",
       " 'locked_for_user': False,\n",
       " 'submissions_download_url': 'https://codeclan.instructure.com/courses/7/assignments/714/submissions?zip=1',\n",
       " 'post_manually': False,\n",
       " 'anonymize_students': False,\n",
       " 'require_lockdown_browser': False,\n",
       " 'restrict_quantitative_data': False}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
